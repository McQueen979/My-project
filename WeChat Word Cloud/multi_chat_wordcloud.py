"""
å¤šå¾®ä¿¡èŠå¤©è®°å½•è¯äº‘å›¾ç”Ÿæˆå™¨
å¯ä»¥ä¸€æ¬¡æ€§å¤„ç†å¤šä¸ªJSONæ–‡ä»¶ï¼Œç”Ÿæˆç»¼åˆè¯äº‘
"""

import json
import re
import jieba
import os
from collections import Counter
from wordcloud import WordCloud
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import glob

# ============================================
# ç¬¬ä¸€éƒ¨åˆ†ï¼šè®¾ç½®å‚æ•°
# ============================================

# 1. æ–‡ä»¶è®¾ç½®
JSON_FOLDER = "."  # JSONæ–‡ä»¶æ‰€åœ¨çš„æ–‡ä»¶å¤¹ï¼Œé»˜è®¤å½“å‰æ–‡ä»¶å¤¹
# æˆ–è€…æŒ‡å®šå…·ä½“çš„æ–‡ä»¶åˆ—è¡¨ï¼ˆäºŒé€‰ä¸€ï¼‰
# JSON_FILES = ["chat1.json", "chat2.json", "chat3.json"]
JSON_FILES = []  # å¦‚æœä¸ºç©ºï¼Œåˆ™è‡ªåŠ¨æŸ¥æ‰¾æ–‡ä»¶å¤¹ä¸‹æ‰€æœ‰JSONæ–‡ä»¶

# 2. æ–‡ä»¶è¿‡æ»¤è®¾ç½®
FILE_PATTERN = "*.json"  # æ–‡ä»¶åŒ¹é…æ¨¡å¼
EXCLUDE_FILES = []  # è¦æ’é™¤çš„æ–‡ä»¶ååˆ—è¡¨

# 3. åˆ†æè®¾ç½®
ANALYZE_WHO = "all"  # "all"=å…¨éƒ¨, "me"=è‡ªå·±, "other"=å¯¹æ–¹
EXCLUDE_SYSTEM_MESSAGES = True
INCLUDE_NAMES_IN_WORDS = False  # æ˜¯å¦å°†èŠå¤©å¯¹è±¡çš„åå­—åŠ å…¥è¯äº‘

# 4. è¯äº‘æ˜¾ç¤ºè®¾ç½®
MAX_WORDS = 250
BACKGROUND_COLOR = "white"
WIDTH = 1200
HEIGHT = 800
FONT_SIZE_RANGE = (8, 120)
FREQUENCY_EXPONENT = 1.8
USE_LOG_SCALE = True
RELATIVE_SCALING = 0.8
COLOR_SCHEME = "viridis"

# 5. è¾“å‡ºè®¾ç½®
OUTPUT_IMAGE = "combined_wordcloud.png"
OUTPUT_STATS = "combined_word_frequency.csv"
OUTPUT_SUMMARY = "chat_summary.csv"  # èŠå¤©è®°å½•æ±‡æ€»ç»Ÿè®¡

# 6. åœç”¨è¯
STOP_WORDS = [
    "çš„", "äº†", "åœ¨", "æ˜¯", "æˆ‘", "æœ‰", "å’Œ", "å°±", 
    "ä¸", "äºº", "éƒ½", "ä¸€", "ä¸€ä¸ª", "ä¸Š", "ä¹Ÿ", "å¾ˆ", 
    "åˆ°", "è¯´", "è¦", "å»", "ä½ ", "ä¼š", "ç€", "æ²¡æœ‰", 
    "çœ‹", "å¥½", "è‡ªå·±", "è¿™", "ä¸­", "å°±æ˜¯", "å¯¹", "åœ¨", 
    "å¯ä»¥", "å§", "å•¦", "å—", "å‘¢", "å•Š", "å‘€", "å“¦",
    "å“ˆå“ˆ", "å“ˆå“ˆå“ˆ", "å“ˆå“ˆå“ˆå“ˆ", "å˜»å˜»", "å‘µå‘µ", "å—¯",
    "è¿™ä¸ª", "é‚£ä¸ª", "ä»€ä¹ˆ", "æ€ä¹ˆ", "ä¸ºä»€ä¹ˆ", "å› ä¸º",
    "æ‰€ä»¥", "ä½†æ˜¯", "ç„¶å", "è€Œä¸”", "å…¶å®", "è¿˜æ˜¯",
    "å°±æ˜¯", "å°±æ˜¯", "å°±æ˜¯", "å°±æ˜¯", "å°±æ˜¯", "å°±æ˜¯"
]

# 7. æ’é™¤æ¨¡å¼
REMOVE_PATTERNS = [
    r'http[s]?://\S+',
    r'\[.*?\]',
    r'ã€.*?ã€‘',
    r'#.*?#',
    r'<.*?>',
    r'å¾®ä¿¡.*?è¡¨æƒ…',
    r'è§†é¢‘.*?èŠå¤©',
    r'è¯­éŸ³.*?æ¶ˆæ¯',
]

# ============================================
# ç¬¬äºŒéƒ¨åˆ†ï¼šæ–‡ä»¶å¤„ç†å‡½æ•°
# ============================================

def get_json_files(folder_path, file_pattern="*.json", exclude_files=None):
    """
    è·å–æŒ‡å®šæ–‡ä»¶å¤¹ä¸‹æ‰€æœ‰çš„JSONæ–‡ä»¶
    """
    if exclude_files is None:
        exclude_files = []
    
    # å¦‚æœæŒ‡å®šäº†å…·ä½“çš„æ–‡ä»¶åˆ—è¡¨ï¼Œå°±ä½¿ç”¨å®ƒ
    if JSON_FILES:
        print(f"ä½¿ç”¨æŒ‡å®šçš„æ–‡ä»¶åˆ—è¡¨: {JSON_FILES}")
        valid_files = []
        for file in JSON_FILES:
            if os.path.exists(file):
                valid_files.append(file)
            else:
                print(f"è­¦å‘Šï¼šæ–‡ä»¶ä¸å­˜åœ¨: {file}")
        return valid_files
    
    # å¦åˆ™è‡ªåŠ¨æŸ¥æ‰¾æ–‡ä»¶å¤¹ä¸‹çš„æ‰€æœ‰JSONæ–‡ä»¶
    pattern = os.path.join(folder_path, file_pattern)
    all_files = glob.glob(pattern)
    
    # è¿‡æ»¤æ‰æ’é™¤çš„æ–‡ä»¶
    filtered_files = [f for f in all_files 
                     if os.path.basename(f) not in exclude_files]
    
    # æŒ‰æ–‡ä»¶å¤§å°æ’åºï¼ˆä»å¤§åˆ°å°ï¼‰
    filtered_files.sort(key=lambda x: os.path.getsize(x), reverse=True)
    
    return filtered_files

def load_single_chat_file(file_path):
    """
    åŠ è½½å•ä¸ªèŠå¤©è®°å½•æ–‡ä»¶
    """
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        filename = os.path.basename(file_path)
        
        # æ£€æŸ¥æ•°æ®ç»“æ„
        if 'messages' not in data:
            print(f"è­¦å‘Šï¼š{filename} ä¸­æ²¡æœ‰æ‰¾åˆ°'messages'å­—æ®µï¼Œè·³è¿‡æ­¤æ–‡ä»¶")
            return None, filename
        
        messages = data['messages']
        
        # è·å–èŠå¤©ä¿¡æ¯
        chat_info = {
            'filename': filename,
            'message_count': len(messages),
            'chat_name': 'æœªçŸ¥èŠå¤©',
            'last_time': 'æœªçŸ¥',
            'type': 'æœªçŸ¥'
        }
        
        if 'session' in data:
            session = data['session']
            chat_info['chat_name'] = session.get('nickname', 
                                               session.get('remark', 
                                                         session.get('displayName', 'æœªçŸ¥èŠå¤©')))
            chat_info['last_time'] = session.get('lastTimestamp', 'æœªçŸ¥')
            chat_info['type'] = session.get('type', 'æœªçŸ¥')
            chat_info['message_count'] = session.get('messageCount', len(messages))
        
        print(f"  âœ“ å·²åŠ è½½: {filename}")
        print(f"    èŠå¤©å¯¹è±¡: {chat_info['chat_name']}")
        print(f"    æ¶ˆæ¯æ•°é‡: {chat_info['message_count']}")
        
        return messages, chat_info
        
    except json.JSONDecodeError as e:
        print(f"é”™è¯¯ï¼š{file_path} ä¸æ˜¯æœ‰æ•ˆçš„JSONæ–‡ä»¶ - {e}")
        return None, None
    except Exception as e:
        print(f"è¯»å– {file_path} æ—¶å‡ºé”™: {e}")
        return None, None

def load_all_chat_files():
    """
    åŠ è½½æ‰€æœ‰èŠå¤©è®°å½•æ–‡ä»¶
    """
    print("æ­£åœ¨æ‰«æJSONæ–‡ä»¶...")
    
    # è·å–æ‰€æœ‰JSONæ–‡ä»¶
    json_files = get_json_files(JSON_FOLDER, FILE_PATTERN, EXCLUDE_FILES)
    
    if not json_files:
        print(f"é”™è¯¯ï¼šåœ¨ '{JSON_FOLDER}' æ–‡ä»¶å¤¹ä¸­æ²¡æœ‰æ‰¾åˆ°JSONæ–‡ä»¶ï¼")
        print("è¯·æ£€æŸ¥ï¼š")
        print(f"1. JSONæ–‡ä»¶æ˜¯å¦åœ¨ '{JSON_FOLDER}' æ–‡ä»¶å¤¹ä¸­")
        print(f"2. æ–‡ä»¶æ‰©å±•åæ˜¯å¦ä¸º .json")
        return [], []
    
    print(f"æ‰¾åˆ° {len(json_files)} ä¸ªJSONæ–‡ä»¶:")
    for i, file in enumerate(json_files, 1):
        size_mb = os.path.getsize(file) / (1024 * 1024)
        print(f"  {i:2d}. {os.path.basename(file)} ({size_mb:.1f} MB)")
    
    print("\nå¼€å§‹åŠ è½½æ–‡ä»¶...")
    
    all_messages = []
    chat_infos = []
    skipped_files = []
    
    for file_path in json_files:
        messages, chat_info = load_single_chat_file(file_path)
        
        if messages is not None and chat_info is not None:
            all_messages.extend(messages)
            chat_infos.append(chat_info)
        else:
            skipped_files.append(os.path.basename(file_path))
    
    print(f"\næ–‡ä»¶åŠ è½½å®Œæˆ:")
    print(f"  âœ“ æˆåŠŸåŠ è½½: {len(chat_infos)} ä¸ªæ–‡ä»¶")
    print(f"  âœ— è·³è¿‡æ–‡ä»¶: {len(skipped_files)} ä¸ª")
    if skipped_files:
        print(f"    è·³è¿‡çš„æ–‡ä»¶: {', '.join(skipped_files)}")
    
    return all_messages, chat_infos

# ============================================
# ç¬¬ä¸‰éƒ¨åˆ†ï¼šæ•°æ®å¤„ç†å‡½æ•°
# ============================================

def filter_messages(messages, who="all", exclude_system=True):
    """
    è¿‡æ»¤æ¶ˆæ¯
    """
    filtered = []
    stats = {
        'total': len(messages),
        'text': 0,
        'system': 0,
        'me': 0,
        'other': 0,
        'other_names': set()
    }
    
    for msg in messages:
        msg_type = msg.get('type', '')
        content = msg.get('content', '')
        
        if not content or not isinstance(content, str):
            continue
        
        # ç»Ÿè®¡ç³»ç»Ÿæ¶ˆæ¯
        if msg_type == "ç³»ç»Ÿæ¶ˆæ¯":
            stats['system'] += 1
            if exclude_system:
                continue
        else:
            stats['text'] += 1
        
        # è·å–å‘é€è€…ä¿¡æ¯
        is_send = msg.get('isSend')
        sender_name = msg.get('senderDisplayName', '')
        
        # ç»Ÿè®¡å‘é€è€…
        if is_send == 1:
            stats['me'] += 1
        elif is_send == 0 and sender_name:
            stats['other'] += 1
            stats['other_names'].add(sender_name)
        
        # æ ¹æ®å‘é€è€…è¿‡æ»¤
        if who == "me" and is_send != 1:
            continue
        elif who == "other" and is_send != 0:
            continue
        elif who not in ["all", "me", "other"]:
            continue
        
        # æ·»åŠ æ¶ˆæ¯
        filtered.append({
            'content': content,
            'type': msg_type,
            'sender': sender_name,
            'isSend': is_send,
            'time': msg.get('formattedTime', '')
        })
    
    print(f"\næ¶ˆæ¯ç»Ÿè®¡:")
    print(f"  æ€»æ¶ˆæ¯æ•°: {stats['total']}")
    print(f"  æ–‡æœ¬æ¶ˆæ¯: {stats['text']}")
    print(f"  ç³»ç»Ÿæ¶ˆæ¯: {stats['system']}")
    print(f"  æˆ‘å‘é€çš„: {stats['me']}")
    print(f"  å¯¹æ–¹å‘é€: {stats['other']}")
    if stats['other_names']:
        print(f"  èŠå¤©å¯¹è±¡: {', '.join(stats['other_names'])}")
    
    print(f"è¿‡æ»¤åå¾—åˆ° {len(filtered)} æ¡æœ‰æ•ˆæ¶ˆæ¯")
    return filtered, stats

def clean_text(text, remove_patterns=None):
    """
    æ¸…æ´—æ–‡æœ¬
    """
    if remove_patterns is None:
        remove_patterns = REMOVE_PATTERNS
    
    if not isinstance(text, str):
        return ""
    
    cleaned = text
    for pattern in remove_patterns:
        cleaned = re.sub(pattern, '', cleaned)
    
    # ç§»é™¤å¤šä½™ç©ºç™½å­—ç¬¦
    cleaned = re.sub(r'\s+', ' ', cleaned).strip()
    
    return cleaned

def extract_texts_from_messages(messages, include_names=False, names=None):
    """
    ä»æ¶ˆæ¯ä¸­æå–æ–‡æœ¬
    """
    texts = []
    word_count = 0
    
    for msg in messages:
        content = msg.get('content', '')
        if not content:
            continue
        
        cleaned = clean_text(content)
        if cleaned:
            texts.append(cleaned)
            word_count += len(cleaned)
            
            # å¦‚æœéœ€è¦ï¼Œæ·»åŠ èŠå¤©å¯¹è±¡åå­—åˆ°æ–‡æœ¬ä¸­
            if include_names and names:
                sender = msg.get('sender', '')
                if sender in names:
                    # å°†åå­—æŒ‰å•ä¸ªå­—æ‹†åˆ†ï¼Œé¿å…jiebaåˆ†è¯è¯†åˆ«
                    for char in sender:
                        if char not in STOP_WORDS and len(char) > 0:
                            texts.append(char)
    
    print(f"æå–åˆ° {len(texts)} æ¡æ–‡æœ¬ï¼Œå…±çº¦ {word_count} ä¸ªå­—ç¬¦")
    return texts

# ============================================
# ç¬¬å››éƒ¨åˆ†ï¼šè¯äº‘ç”Ÿæˆå‡½æ•°
# ============================================

def enhance_frequency_distribution(word_counts, exponent=1.5, use_log_scale=True):
    """
    å¢å¼ºè¯é¢‘åˆ†å¸ƒ
    """
    if not word_counts:
        return {}
    
    enhanced_counts = {}
    
    frequencies = list(word_counts.values())
    if len(frequencies) < 2:
        return word_counts
    
    min_freq = min(frequencies)
    max_freq = max(frequencies)
    
    for word, freq in word_counts.items():
        if use_log_scale:
            enhanced = math.log(freq + 1) ** exponent
        else:
            enhanced = freq ** exponent
        enhanced_counts[word] = enhanced
    
    # ç¼©æ”¾åˆ°1-100èŒƒå›´
    enhanced_values = list(enhanced_counts.values())
    min_enhanced = min(enhanced_values)
    max_enhanced = max(enhanced_values)
    
    scaled_counts = {}
    for word, enhanced in enhanced_counts.items():
        if max_enhanced > min_enhanced:
            scaled = 1 + 99 * (enhanced - min_enhanced) / (max_enhanced - min_enhanced)
        else:
            scaled = 50
        scaled_counts[word] = scaled
    
    return scaled_counts

def generate_combined_wordcloud(texts, chat_infos=None):
    """
    ç”Ÿæˆç»¼åˆè¯äº‘
    """
    if not texts:
        print("é”™è¯¯ï¼šæ²¡æœ‰æ–‡æœ¬å¯ä»¥ç”Ÿæˆè¯äº‘")
        return None, None
    
    print(f"\næ­£åœ¨å¤„ç† {len(texts)} æ¡æ–‡æœ¬...")
    
    # åˆå¹¶æ‰€æœ‰æ–‡æœ¬
    all_text = ' '.join(texts)
    
    # ä½¿ç”¨jiebaåˆ†è¯
    print("æ­£åœ¨åˆ†è¯...")
    words = jieba.lcut(all_text)
    
    # è¿‡æ»¤åœç”¨è¯å’Œå•å­—
    filtered_words = []
    for word in words:
        word = word.strip()
        if (len(word) > 1 and
            word not in STOP_WORDS and
            not word.isdigit() and
            not re.match(r'^[^\u4e00-\u9fa5]+$', word)):
            filtered_words.append(word)
    
    # ç»Ÿè®¡è¯é¢‘
    word_counts = Counter(filtered_words)
    
    if not word_counts:
        print("é”™è¯¯ï¼šåˆ†è¯åæ²¡æœ‰æœ‰æ•ˆçš„è¯è¯­")
        return None, None
    
    print(f"åˆ†è¯å¾—åˆ° {len(filtered_words)} ä¸ªæœ‰æ•ˆè¯è¯­ï¼Œ{len(word_counts)} ä¸ªä¸åŒè¯è¯­")
    
    # æ˜¾ç¤ºæœ€å¸¸è§çš„30ä¸ªè¯
    print("\næœ€å¸¸è§çš„30ä¸ªè¯è¯­:")
    for i, (word, count) in enumerate(word_counts.most_common(30), 1):
        print(f"  {i:2d}. {word:10s}: {count:6d}æ¬¡")
    
    # å¢å¼ºè¯é¢‘åˆ†å¸ƒ
    print(f"\nåº”ç”¨å¢å¼ºå‚æ•°: æŒ‡æ•°={FREQUENCY_EXPONENT}, å¯¹æ•°ç¼©æ”¾={USE_LOG_SCALE}")
    enhanced_counts = enhance_frequency_distribution(
        word_counts, 
        exponent=FREQUENCY_EXPONENT,
        use_log_scale=USE_LOG_SCALE
    )
    
    # æŸ¥æ‰¾å­—ä½“
    font_path = None
    possible_fonts = [
        "C:/Windows/Fonts/simhei.ttf",
        "C:/Windows/Fonts/msyh.ttc",
        "C:/Windows/Fonts/simsun.ttc",
        "simhei.ttf",
    ]
    
    for font in possible_fonts:
        if os.path.exists(font):
            font_path = font
            print(f"ä½¿ç”¨å­—ä½“: {font}")
            break
    
    if not font_path:
        print("è­¦å‘Šï¼šæœªæ‰¾åˆ°ä¸­æ–‡å­—ä½“ï¼Œä½¿ç”¨é»˜è®¤å­—ä½“")
    
    # åˆ›å»ºè¯äº‘å¯¹è±¡
    print("æ­£åœ¨ç”Ÿæˆè¯äº‘...")
    wc = WordCloud(
        font_path=font_path,
        width=WIDTH,
        height=HEIGHT,
        background_color=BACKGROUND_COLOR,
        max_words=MAX_WORDS,
        max_font_size=FONT_SIZE_RANGE[1],
        min_font_size=FONT_SIZE_RANGE[0],
        relative_scaling=RELATIVE_SCALING,
        random_state=42,
        collocations=False,
        colormap=COLOR_SCHEME,
        prefer_horizontal=0.9,
        scale=2,
        contour_width=0,
        contour_color='steelblue',
    )
    
    # ç”Ÿæˆè¯äº‘
    wc.generate_from_frequencies(enhanced_counts)
    
    print(f"è¯äº‘ç”Ÿæˆå®Œæˆï¼ŒåŒ…å« {len(wc.words_)} ä¸ªè¯è¯­")
    
    return wc, word_counts

# ============================================
# ç¬¬äº”éƒ¨åˆ†ï¼šç»“æœä¿å­˜å’Œæ˜¾ç¤º
# ============================================

def save_combined_results(wordcloud, word_counts, chat_infos, stats):
    """
    ä¿å­˜ç»¼åˆç»“æœ
    """
    if not wordcloud:
        return False
    
    # ä¿å­˜è¯äº‘å›¾ç‰‡
    try:
        wordcloud.to_file(OUTPUT_IMAGE)
        print(f"\nè¯äº‘å›¾ç‰‡å·²ä¿å­˜: {OUTPUT_IMAGE}")
    except Exception as e:
        print(f"ä¿å­˜å›¾ç‰‡å¤±è´¥: {e}")
        return False
    
    # ä¿å­˜è¯é¢‘ç»Ÿè®¡
    try:
        df_word_freq = pd.DataFrame(
            word_counts.most_common(),
            columns=['è¯è¯­', 'é¢‘æ¬¡']
        )
        df_word_freq.to_csv(OUTPUT_STATS, index=False, encoding='utf-8-sig')
        print(f"è¯é¢‘ç»Ÿè®¡å·²ä¿å­˜: {OUTPUT_STATS}")
        
        # æ˜¾ç¤ºå‰20ä¸ªè¯
        print("\nè¯é¢‘å‰20å:")
        for i, row in df_word_freq.head(20).iterrows():
            print(f"  {i+1:2d}. {row['è¯è¯­']:10s}: {row['é¢‘æ¬¡']:6d}æ¬¡")
        
        # ç»Ÿè®¡ä¿¡æ¯
        if len(df_word_freq) >= 2:
            max_word = df_word_freq.iloc[0]['è¯è¯­']
            max_freq = df_word_freq.iloc[0]['é¢‘æ¬¡']
            min_word = df_word_freq.iloc[-1]['è¯è¯­']
            min_freq = df_word_freq.iloc[-1]['é¢‘æ¬¡']
            ratio = max_freq / min_freq if min_freq > 0 else 0
            
            print(f"\nè¯é¢‘å·®å¼‚ç»Ÿè®¡:")
            print(f"  æœ€é«˜é¢‘è¯ '{max_word}': {max_freq} æ¬¡")
            print(f"  æœ€ä½é¢‘è¯ '{min_word}': {min_freq} æ¬¡")
            print(f"  é¢‘æ¬¡æ¯”: {ratio:.1f}:1")
            
    except Exception as e:
        print(f"ä¿å­˜è¯é¢‘ç»Ÿè®¡å¤±è´¥: {e}")
    
    # ä¿å­˜èŠå¤©è®°å½•æ±‡æ€»
    try:
        if chat_infos:
            df_summary = pd.DataFrame(chat_infos)
            df_summary = df_summary[['filename', 'chat_name', 'message_count', 'type', 'last_time']]
            df_summary.to_csv(OUTPUT_SUMMARY, index=False, encoding='utf-8-sig')
            print(f"èŠå¤©æ±‡æ€»å·²ä¿å­˜: {OUTPUT_SUMMARY}")
            
            print("\nèŠå¤©è®°å½•æ±‡æ€»:")
            for i, info in enumerate(chat_infos, 1):
                print(f"  {i:2d}. {info['chat_name']:20s} ({info['filename']}): {info['message_count']} æ¡æ¶ˆæ¯")
    
    except Exception as e:
        print(f"ä¿å­˜èŠå¤©æ±‡æ€»å¤±è´¥: {e}")
    
    return True

def display_combined_wordcloud(wordcloud, word_counts, chat_infos, stats):
    """
    æ˜¾ç¤ºç»¼åˆè¯äº‘
    """
    if not wordcloud:
        return
    
    # åˆ›å»ºå¤§å›¾
    fig = plt.figure(figsize=(18, 10))
    
    # 1. è¯äº‘å›¾
    ax1 = plt.subplot2grid((2, 3), (0, 0), colspan=2, rowspan=2)
    ax1.imshow(wordcloud, interpolation="bilinear")
    ax1.axis("off")
    
    # æ·»åŠ æ ‡é¢˜
    if chat_infos:
        chat_names = [info['chat_name'] for info in chat_infos]
        title = f"ç»¼åˆè¯äº‘å›¾ - å…±{len(chat_infos)}ä¸ªèŠå¤©è®°å½•"
        if len(chat_names) <= 5:
            title += f"\n({', '.join(chat_names)})"
        else:
            title += f"\n({', '.join(chat_names[:3])} ç­‰)"
    else:
        title = "å¾®ä¿¡èŠå¤©è®°å½•ç»¼åˆè¯äº‘å›¾"
    
    ax1.set_title(title, fontsize=16, fontweight='bold', pad=20)
    
    # 2. é«˜é¢‘è¯æŸ±çŠ¶å›¾
    ax2 = plt.subplot2grid((2, 3), (0, 2))
    if word_counts and len(word_counts) > 0:
        top_words = dict(word_counts.most_common(15))
        words = list(top_words.keys())
        freqs = list(top_words.values())
        
        y_pos = range(len(words))
        bars = ax2.barh(y_pos, freqs, align='center', alpha=0.7, color='steelblue')
        ax2.set_yticks(y_pos)
        ax2.set_yticklabels(words, fontproperties='SimHei')
        ax2.invert_yaxis()
        ax2.set_xlabel('å‡ºç°æ¬¡æ•°')
        ax2.set_title('é«˜é¢‘è¯Top 15')
        
        # æ·»åŠ æ•°å€¼
        for i, (bar, freq) in enumerate(zip(bars, freqs)):
            width = bar.get_width()
            ax2.text(width + max(freqs)*0.01, bar.get_y() + bar.get_height()/2,
                    f'{freq}', va='center', fontsize=9)
    
    # 3. èŠå¤©è®°å½•ç»Ÿè®¡
    ax3 = plt.subplot2grid((2, 3), (1, 2))
    
    if chat_infos and len(chat_infos) > 0:
        # åªæ˜¾ç¤ºå‰10ä¸ªèŠå¤©çš„æ¶ˆæ¯æ•°
        display_infos = chat_infos[:10]
        chat_labels = [info['chat_name'] for info in display_infos]
        message_counts = [info['message_count'] for info in display_infos]
        
        y_pos = range(len(chat_labels))
        bars = ax3.barh(y_pos, message_counts, align='center', alpha=0.7, color='lightcoral')
        ax3.set_yticks(y_pos)
        ax3.set_yticklabels(chat_labels, fontproperties='SimHei', fontsize=9)
        ax3.invert_yaxis()
        ax3.set_xlabel('æ¶ˆæ¯æ•°é‡')
        ax3.set_title('èŠå¤©è®°å½•ç»Ÿè®¡')
        
        # æ·»åŠ æ€»è®¡
        total_messages = sum(message_counts)
        if len(chat_infos) > 10:
            ax3.text(0.98, 0.02, f"æ€»è®¡: {total_messages} æ¡\n(æ˜¾ç¤ºå‰10ä¸ª)",
                    transform=ax3.transAxes, ha='right', fontsize=9,
                    bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))
        else:
            ax3.text(0.98, 0.02, f"æ€»è®¡: {total_messages} æ¡",
                    transform=ax3.transAxes, ha='right', fontsize=9,
                    bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))
    
    plt.suptitle(f"å¤šèŠå¤©è®°å½•è¯äº‘åˆ†æ - å…± {len(word_counts)} ä¸ªä¸åŒè¯è¯­", fontsize=18, y=0.98)
    plt.tight_layout()
    plt.show()

# ============================================
# ç¬¬å…­éƒ¨åˆ†ï¼šä¸»ç¨‹åº
# ============================================

import math  # ç”¨äºenhance_frequency_distributionå‡½æ•°ä¸­çš„math.log

def main():
    """
    ä¸»å‡½æ•°
    """
    print("=" * 70)
    print("å¤šå¾®ä¿¡èŠå¤©è®°å½•è¯äº‘å›¾ç”Ÿæˆå™¨")
    print("=" * 70)
    print(f"æœç´¢æ–‡ä»¶å¤¹: {JSON_FOLDER}")
    print(f"æ–‡ä»¶æ¨¡å¼: {FILE_PATTERN}")
    print(f"åˆ†æå¯¹è±¡: {ANALYZE_WHO}")
    print(f"æ’é™¤ç³»ç»Ÿæ¶ˆæ¯: {EXCLUDE_SYSTEM_MESSAGES}")
    print("=" * 70)
    
    # 1. åŠ è½½æ‰€æœ‰èŠå¤©è®°å½•
    all_messages, chat_infos = load_all_chat_files()
    
    if not all_messages or not chat_infos:
        print("é”™è¯¯ï¼šæ²¡æœ‰å¯ç”¨çš„èŠå¤©è®°å½•æ•°æ®")
        input("æŒ‰å›è½¦é”®é€€å‡º...")
        return
    
    print(f"\nâœ“ æˆåŠŸåŠ è½½ {len(chat_infos)} ä¸ªèŠå¤©è®°å½•ï¼Œå…± {len(all_messages)} æ¡æ¶ˆæ¯")
    
    # 2. è¿‡æ»¤æ¶ˆæ¯
    print(f"\næ­£åœ¨è¿‡æ»¤æ¶ˆæ¯ (åˆ†æå¯¹è±¡: {ANALYZE_WHO})...")
    filtered_messages, stats = filter_messages(
        all_messages, 
        who=ANALYZE_WHO,
        exclude_system=EXCLUDE_SYSTEM_MESSAGES
    )
    
    if not filtered_messages:
        print("è¿‡æ»¤åæ²¡æœ‰æ¶ˆæ¯å¯åˆ†æ")
        input("æŒ‰å›è½¦é”®é€€å‡º...")
        return
    
    # 3. æå–æ–‡æœ¬
    print("\næ­£åœ¨æå–å’Œæ¸…æ´—æ–‡æœ¬...")
    
    # è·å–æ‰€æœ‰èŠå¤©å¯¹è±¡çš„åå­—
    other_names = set()
    for msg in filtered_messages:
        if msg.get('isSend') == 0:  # å¯¹æ–¹å‘é€
            sender = msg.get('sender', '')
            if sender:
                other_names.add(sender)
    
    texts = extract_texts_from_messages(
        filtered_messages, 
        include_names=INCLUDE_NAMES_IN_WORDS,
        names=other_names
    )
    
    if not texts:
        print("é”™è¯¯ï¼šæ²¡æœ‰æå–åˆ°æœ‰æ•ˆæ–‡æœ¬")
        input("æŒ‰å›è½¦é”®é€€å‡º...")
        return
    
    # 4. ç”Ÿæˆç»¼åˆè¯äº‘
    wordcloud, word_counts = generate_combined_wordcloud(texts, chat_infos)
    
    if not wordcloud:
        print("ç”Ÿæˆè¯äº‘å¤±è´¥")
        input("æŒ‰å›è½¦é”®é€€å‡º...")
        return
    
    # 5. ä¿å­˜å’Œæ˜¾ç¤ºç»“æœ
    print("\næ­£åœ¨ä¿å­˜ç»“æœ...")
    save_combined_results(wordcloud, word_counts, chat_infos, stats)
    
    print("\næ­£åœ¨æ˜¾ç¤ºè¯äº‘å›¾...")
    display_combined_wordcloud(wordcloud, word_counts, chat_infos, stats)
    
    print("\n" + "=" * 70)
    print("å¤„ç†å®Œæˆï¼")
    print("=" * 70)
    print(f"ğŸ“ åˆ†ææ–‡ä»¶: {len(chat_infos)} ä¸ªèŠå¤©è®°å½•")
    print(f"ğŸ’¬ æ€»æ¶ˆæ¯æ•°: {len(all_messages)} æ¡")
    print(f"ğŸ“Š æœ‰æ•ˆæ¶ˆæ¯: {len(filtered_messages)} æ¡")
    print(f"ğŸ”¤ ä¸åŒè¯è¯­: {len(word_counts)} ä¸ª")
    print(f"ğŸ–¼ï¸  è¯äº‘å›¾ç‰‡: {OUTPUT_IMAGE}")
    print(f"ğŸ“ˆ è¯é¢‘ç»Ÿè®¡: {OUTPUT_STATS}")
    if chat_infos:
        print(f"ğŸ“‹ èŠå¤©æ±‡æ€»: {OUTPUT_SUMMARY}")
    print("=" * 70)
    
    input("æŒ‰å›è½¦é”®é€€å‡ºç¨‹åº...")

# è¿è¡Œä¸»ç¨‹åº
if __name__ == "__main__":
    main()